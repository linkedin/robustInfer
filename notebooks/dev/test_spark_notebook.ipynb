{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c6601bd3-ff33-44d3-a097-7a88d06cc0de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3.4.1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark\n",
    "spark.version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4e5fbef3-3d22-43a5-9e35-9327ce4c09fa",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "sess = org.apache.spark.sql.SparkSession@490f0576\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "org.apache.spark.sql.SparkSession@490f0576"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.stat.ChiSquareTest\n",
    "val sess = SparkSession.builder()\n",
    "  .appName(\"MyNotebook\")\n",
    "  .master(\"local[*]\")\n",
    "  .config(\"spark.jars.packages\", \"org.apache.spark:spark-mllib_2.13:3.4.1\")\n",
    "  .getOrCreate()\n",
    "\n",
    "import sess.implicits._\n",
    "import org.apache.spark.ml.linalg.Vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "590e7cb0-004b-41ef-af0e-0fa45c4d323e",
   "metadata": {},
   "source": [
    "## test native chi square"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c988b4f9-71c5-4218-8306-682e9168cc15",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "df = [label: double, features: vector]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[label: double, features: vector]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val df = Seq(\n",
    "  (0.0, Vectors.dense(1.0, 0.0)),\n",
    "  (1.0, Vectors.dense(0.0, 1.0)),\n",
    "  (0.0, Vectors.dense(1.0, 1.0))\n",
    ").toDF(\"label\", \"features\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bedf6f3-23a0-4558-a01f-cc8117061fcc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+\n",
      "|label| features|\n",
      "+-----+---------+\n",
      "|  0.0|[1.0,0.0]|\n",
      "|  1.0|[0.0,1.0]|\n",
      "|  0.0|[1.0,1.0]|\n",
      "+-----+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e095aae1-486b-4f32-b03d-5e8953d67933",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "chi = [pValues: vector, degreesOfFreedom: array<int> ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[pValues: vector, degreesOfFreedom: array<int> ... 1 more field]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import org.apache.spark.ml.stat.ChiSquareTest\n",
    "\n",
    "val chi    = ChiSquareTest.test(df, \"features\", \"label\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fec0d889-ccd5-42fa-af02-5d808fe78f2e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+----------------+--------------------+\n",
      "|             pValues|degreesOfFreedom|          statistics|\n",
      "+--------------------+----------------+--------------------+\n",
      "|[0.08326451666354...|          [1, 1]|[3.00000000000000...|\n",
      "+--------------------+----------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "chi.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a7da517-83b9-42eb-8268-0671b3ded374",
   "metadata": {},
   "source": [
    "## test two sample tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "55036303-eb3d-4e4a-9daf-2bc65549027e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data = [response: double, treatment: int]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[response: double, treatment: int]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data = Seq(\n",
    "      (5.1, 0), (4.9, 0), (5.0, 0), (5.2, 0), (5.3, 0),    // treatment = 0\n",
    "      (6.1, 1), (6.3, 1), (6.5, 1), (6.2, 1), (6.4, 1)     // treatment = 1\n",
    "    ).toDF(\"response\", \"treatment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f83ca128-603d-449e-a8d1-ca173ed0cd1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|response|treatment|\n",
      "+--------+---------+\n",
      "|     5.1|        0|\n",
      "|     4.9|        0|\n",
      "|     5.0|        0|\n",
      "|     5.2|        0|\n",
      "|     5.3|        0|\n",
      "|     6.1|        1|\n",
      "|     6.3|        1|\n",
      "|     6.5|        1|\n",
      "|     6.2|        1|\n",
      "|     6.4|        1|\n",
      "+--------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "97ef19c5-2fe8-4be5-9240-f7afcadacbe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import org.apache.spark.sql.{DataFrame, SparkSession}\n",
    "import org.apache.spark.sql.functions._\n",
    "import org.apache.spark.sql.expressions.Window\n",
    "import org.apache.spark.sql.types.DoubleType\n",
    "import org.apache.commons.math3.distribution.NormalDistribution\n",
    "import org.apache.spark.rdd.RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "bc13b6e2-c3f7-40a5-bffd-0f71069ca12c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "defined object TwoSample\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    }
   ],
   "source": [
    "object TwoSample {\n",
    "\n",
    "  def zeroTrimmedU(\n",
    "    xRdd: RDD[Double],\n",
    "    yRdd: RDD[Double],\n",
    "    alpha: Double = 0.05,\n",
    "    scale: Boolean = true\n",
    "  ): (Double, Double, Double, (Double, Double)) = {\n",
    "    // 1) Basic counts & checks\n",
    "    val n0 = xRdd.count.toDouble\n",
    "    val n1 = yRdd.count.toDouble\n",
    "    require(n0 > 0 && n1 > 0, \"Both RDDs must be non-empty\")\n",
    "    require(xRdd.filter(_ < 0).isEmpty(), \"All x must be ≥ 0\")\n",
    "    require(yRdd.filter(_ < 0).isEmpty(), \"All y must be ≥ 0\")\n",
    "\n",
    "    // 2) Proportions of non-zeros\n",
    "    val xPlus = xRdd.filter(_ > 0)\n",
    "    val yPlus = yRdd.filter(_ > 0)\n",
    "    val pHat0 = xPlus.count / n0\n",
    "    val pHat1 = yPlus.count / n1\n",
    "    val pHat = math.max(pHat0, pHat1)\n",
    "\n",
    "    // 3) Truncate zeros\n",
    "    val nPrime0 = math.round(n0 * pHat).toInt\n",
    "    val nPrime1 = math.round(n1 * pHat).toInt\n",
    "    val nPlus0 = xPlus.count.toDouble\n",
    "    val nPlus1 = yPlus.count.toDouble\n",
    "    val pad0 = Seq.fill(nPrime0 - nPlus0.toInt)(0.0)\n",
    "    val pad1 = Seq.fill(nPrime1 - nPlus1.toInt)(0.0)\n",
    "\n",
    "    val xTrun = xRdd.sparkContext.parallelize(pad0) union xPlus\n",
    "    val yTrun = yRdd.sparkContext.parallelize(pad1) union yPlus\n",
    "\n",
    "    // 4) Compute descending‐ordinal ranks\n",
    "    val tagged: RDD[(Double, Boolean)] =\n",
    "      yTrun.map(v => (v, true)) union xTrun.map(v => (v, false))\n",
    "\n",
    "    val ranks: RDD[((Double, Boolean), Long)] =\n",
    "      tagged.sortBy({ case (v, _) => -v }).zipWithIndex()\n",
    "\n",
    "    val R1: Double =\n",
    "      ranks.filter { case ((_, isY), _) => isY }\n",
    "        .map { case (_, idx) => (idx + 1).toDouble }\n",
    "        .sum()\n",
    "\n",
    "    // 5) Wilcoxon-style statistic\n",
    "    val wPrime = - (R1 - nPrime1 * (nPrime0 + nPrime1 + 1) / 2.0)\n",
    "\n",
    "    // 6) Variance components\n",
    "    val varComp1 = (n1 * n0 * n1 * n0 / 4.0) * (pHat * pHat) * (\n",
    "      (pHat0 * (1 - pHat0) / n0) + (pHat1 * (1 - pHat1) / n1)\n",
    "    )\n",
    "    val varComp2 = (nPlus0 * nPlus1 * (nPlus0 + nPlus1)) / 12.0\n",
    "    val varW = varComp1 + varComp2\n",
    "\n",
    "    // 7) Z and p-value\n",
    "    val z = wPrime / math.sqrt(varW)\n",
    "    val pValue = 2 * (1 - normalCDF(z))\n",
    "    val zAlpha = normalQuantile(1 - alpha / 2)\n",
    "    val confidenceInterval = (wPrime - zAlpha * math.sqrt(varW), wPrime + zAlpha * math.sqrt(varW))\n",
    "\n",
    "    // 8) Scale the statistic to P(X' < Y')\n",
    "    if (scale) {\n",
    "      val locationFactor = (nPrime1.toDouble * nPrime0.toDouble) * 0.5\n",
    "      val scaleFactor = 1.0 * nPrime1.toDouble * nPrime0.toDouble\n",
    "      val wPrimeScaled = (wPrime + locationFactor)/scaleFactor\n",
    "      val confidenceIntervalScaled = (\n",
    "        (confidenceInterval._1 + locationFactor) / scaleFactor,\n",
    "        (confidenceInterval._2 + locationFactor) / scaleFactor\n",
    "      )\n",
    "      return (z, pValue, wPrimeScaled, confidenceIntervalScaled)\n",
    "    }\n",
    "\n",
    "    (z, pValue, wPrime, confidenceInterval)\n",
    "  }\n",
    "\n",
    "  def mwU(\n",
    "    xRdd: RDD[Double],\n",
    "    yRdd: RDD[Double],\n",
    "    alpha: Double = 0.05,\n",
    "    scale: Boolean = true\n",
    "  ): (Double, Double, Double, (Double, Double)) = {\n",
    "    // 1) Basic counts & checks\n",
    "    val n0 = xRdd.count.toDouble\n",
    "    val n1 = yRdd.count.toDouble\n",
    "    require(n0 > 0 && n1 > 0, \"Both RDDs must be non-empty\")\n",
    "\n",
    "    // 2) Compute descending‐ordinal ranks\n",
    "    val tagged: RDD[(Double, Boolean)] =\n",
    "      yRdd.map(v => (v, true)) union xRdd.map(v => (v, false))\n",
    "\n",
    "    val ranks: RDD[((Double, Boolean), Long)] =\n",
    "      tagged.sortBy({ case (v, _) => -v }).zipWithIndex()\n",
    "\n",
    "    val R1: Double =\n",
    "      ranks.filter { case ((_, isY), _) => isY }\n",
    "        .map { case (_, idx) => (idx + 1).toDouble }\n",
    "        .sum()\n",
    "    \n",
    "    // 3) Wilcoxon-style statistic\n",
    "    val w = - (R1 - n1 * (n0 + n1 + 1) / 2.0)\n",
    "\n",
    "    // 4) Variance\n",
    "    val varW = n0 * n1 * (n0 + n1 + 1) / 12.0\n",
    "\n",
    "    // 5) Z and p-value\n",
    "    val z = w / math.sqrt(varW)\n",
    "    val pValue = 2 * (1 - normalCDF(z))\n",
    "    val zAlpha = normalQuantile(1 - alpha / 2)\n",
    "    val confidenceInterval = (w - zAlpha * math.sqrt(varW), w + zAlpha * math.sqrt(varW))\n",
    "\n",
    "    // 6) Scale the statistic to P(X' < Y')\n",
    "    if (scale) {\n",
    "      val locationFactor = (n1 * n0) / 2.0\n",
    "      val scaleFactor = n1 * n0\n",
    "      val wScaled = (w + locationFactor) / scaleFactor\n",
    "      val confidenceIntervalScaled = (\n",
    "        (confidenceInterval._1 + locationFactor) / scaleFactor,\n",
    "        (confidenceInterval._2 + locationFactor) / scaleFactor\n",
    "      )\n",
    "      return (z, pValue, wScaled, confidenceIntervalScaled)\n",
    "    }\n",
    "    \n",
    "    (z, pValue, w, confidenceInterval)\n",
    "  }\n",
    "\n",
    "  def tTest(\n",
    "    xRdd: RDD[Double],\n",
    "    yRdd: RDD[Double],\n",
    "    alpha: Double = 0.05\n",
    "  ): (Double, Double, Double, (Double, Double)) = {\n",
    "    // This function performs a two-sample t-test on two RDDs of doubles.\n",
    "    // 1) Basic counts & checks\n",
    "    val n0 = xRdd.count.toDouble\n",
    "    val n1 = yRdd.count.toDouble\n",
    "    require(n0 > 0 && n1 > 0, \"Both RDDs must be non-empty\")\n",
    "\n",
    "    // 2) Calculate means, variances, and counts for each group\n",
    "    val mean0 = xRdd.mean()\n",
    "    val mean1 = yRdd.mean()\n",
    "    val var0 = xRdd.variance()\n",
    "    val var1 = yRdd.variance()\n",
    "\n",
    "    // 3) Perform the t-test\n",
    "    val stdErrorDifference = math.sqrt(var0 / n0 + var1 / n1)\n",
    "    val z = (mean0 - mean1) / stdErrorDifference\n",
    "\n",
    "    // 4) Calculate the p-value using the normal distribution CDF\n",
    "    val pValue = 2 * (1 - normalCDF(math.abs(z)))\n",
    "\n",
    "    // 5) Calculate the 95% confidence interval for the mean difference\n",
    "    val meanDifference = mean1 - mean0\n",
    "    val zAlpha = normalQuantile(1 - alpha / 2)\n",
    "    val confidenceInterval = (meanDifference - zAlpha * stdErrorDifference, meanDifference + zAlpha * stdErrorDifference)\n",
    "\n",
    "    (z, pValue, meanDifference, confidenceInterval)\n",
    "  }\n",
    "\n",
    "  def zeroTrimmedUDf(data: DataFrame, groupCol: String, valueCol: String,\n",
    "    controlStr: String, treatmentStr: String, alpha: Double): (Double, Double, Double, (Double, Double)) = {\n",
    "    // This test basically test P(X < Y) = 0.5, where X is a random variable from control group and Y is a random variable from treatment group\n",
    "    // Filter and select the relevant data\n",
    "    val filteredData = data\n",
    "      .withColumn(valueCol, col(valueCol).cast(DoubleType))\n",
    "      .filter(col(groupCol).isin(controlStr, treatmentStr))\n",
    "\n",
    "    val summary = filteredData.groupBy(groupCol).agg(\n",
    "      sum(when(col(valueCol) > 0, 1.0).otherwise(col(valueCol))).as(\"positiveCount\"),\n",
    "      mean(when(col(valueCol) > 0, 1.0).otherwise(col(valueCol))).as(\"theta\"),\n",
    "      count(valueCol).alias(\"count\"))\n",
    "    \n",
    "    val n0Plus = summary.filter(col(groupCol) === controlStr).first().getDouble(1)\n",
    "    val p0Hat = summary.filter(col(groupCol) === controlStr).first().getDouble(2)\n",
    "    val n0 = summary.filter(col(groupCol) === controlStr).first().getLong(3)\n",
    "\n",
    "    val n1Plus = summary.filter(col(groupCol) === treatmentStr).first().getDouble(1)\n",
    "    val p1Hat = summary.filter(col(groupCol) === treatmentStr).first().getDouble(2)\n",
    "    val n1 = summary.filter(col(groupCol) === treatmentStr).first().getLong(3)\n",
    "\n",
    "    val pHat = if (p0Hat > p1Hat) p0Hat else p1Hat\n",
    "    val samplingGrpStr = if (p0Hat > p1Hat) treatmentStr else controlStr\n",
    "    val samplingSize = math.round(math.abs(p0Hat - p1Hat) * (if (p0Hat > p1Hat) n1 else n0)).toInt\n",
    "    val zeroData = filteredData.filter(col(groupCol) === samplingGrpStr).filter(col(valueCol) === 0).limit(samplingSize)\n",
    "    val positiveData = filteredData.filter(col(valueCol) > 0)\n",
    "    val trimmedData = positiveData.union(zeroData)\n",
    "    trimmedData.cache()\n",
    "\n",
    "    val rankedData = trimmedData.withColumn(\"rank\", row_number().over(Window.orderBy(desc(valueCol))))\n",
    "      .withColumn(\"rankD\", col(\"rank\").cast(DoubleType))\n",
    "    val r1 = rankedData.filter(col(groupCol) === treatmentStr).agg(sum(\"rankD\")).first().getDouble(0)\n",
    "    val n0Prime = trimmedData.filter(col(groupCol) === controlStr).count().toDouble\n",
    "    val n1Prime = trimmedData.filter(col(groupCol) === treatmentStr).count().toDouble\n",
    "    trimmedData.unpersist()\n",
    "\n",
    "    val wPrime = - r1 + n1Prime * (n1Prime + n0Prime + 1) / 2\n",
    "\n",
    "    val varComp1 = math.pow(n0, 2) * math.pow(n1, 2) / 4 *\n",
    "      math.pow(pHat, 2) *\n",
    "      ((p0Hat * (1 - p0Hat)) / n0 + (p1Hat * (1 - p1Hat)) / n1)\n",
    "    val varComp2 = n1Plus * n0Plus * (n1Plus + n0Plus) / 12\n",
    "    val varW = varComp1 + varComp2\n",
    "\n",
    "    val z = wPrime / math.sqrt(varW)\n",
    "\n",
    "    // Calculate the p-value using the normal distribution CDF\n",
    "    val pValue = 2 * (1 - normalCDF(z))\n",
    "    val zAlpha = normalQuantile(1 - alpha / 2)\n",
    "    val confidenceInterval = (wPrime - zAlpha * math.sqrt(varW), wPrime + zAlpha * math.sqrt(varW))\n",
    "\n",
    "    (z, pValue, wPrime, confidenceInterval)\n",
    "  }\n",
    "\n",
    "  def tTestDf(data: DataFrame, groupCol: String, valueCol: String,\n",
    "    controlStr: String, treatmentStr: String, alpha: Double): (Double, Double, Double, (Double, Double)) = {\n",
    "    // Filter and select the relevant data\n",
    "    val filteredData = data\n",
    "      .withColumn(valueCol, col(valueCol).cast(DoubleType))\n",
    "      .filter(col(groupCol).isin(controlStr, treatmentStr))\n",
    "\n",
    "    // Calculate means, variances, and counts for each group\n",
    "    val summary = filteredData.groupBy(groupCol).agg(\n",
    "      mean(valueCol).alias(\"mean\"),\n",
    "      variance(valueCol).alias(\"variance\"),\n",
    "      count(valueCol).alias(\"count\")\n",
    "    )\n",
    "\n",
    "    // Extract mean, variance, and count for control and treatment\n",
    "    val controlMean = summary.filter(col(groupCol) === controlStr).first().getDouble(1)\n",
    "    val controlVariance = summary.filter(col(groupCol) === controlStr).first().getDouble(2)\n",
    "    val controlCount = summary.filter(col(groupCol) === controlStr).first().getLong(3)\n",
    "\n",
    "    val treatmentMean = summary.filter(col(groupCol) === treatmentStr).first().getDouble(1)\n",
    "    val treatmentVariance = summary.filter(col(groupCol) === treatmentStr).first().getDouble(2)\n",
    "    val treatmentCount = summary.filter(col(groupCol) === treatmentStr).first().getLong(3)\n",
    "\n",
    "    // Perform the t-test\n",
    "    val stdErrorDifference = math.sqrt(controlVariance/ controlCount + treatmentVariance / treatmentCount)\n",
    "    val t = math.abs(controlMean - treatmentMean) / stdErrorDifference\n",
    "\n",
    "    // Calculate the p-value using the normal distribution CDF\n",
    "    val pValue = 2 * (1 - normalCDF(t))\n",
    "\n",
    "    // Calculate the 95% confidence interval for the mean difference\n",
    "    val meanDifference = treatmentMean - controlMean\n",
    "    val zAlpha = normalQuantile(1 - alpha / 2)\n",
    "    val confidenceInterval = (meanDifference - zAlpha * stdErrorDifference, meanDifference + zAlpha * stdErrorDifference)\n",
    "\n",
    "    (t, pValue, meanDifference, confidenceInterval)\n",
    "  }\n",
    "\n",
    "  // Custom implementation of the normal distribution cumulative distribution function (CDF)\n",
    "  def normalCDF(t: Double): Double = {\n",
    "    val standardNormal = new NormalDistribution(0, 1)\n",
    "    standardNormal.cumulativeProbability(Math.abs(t))\n",
    "  }\n",
    "  // Custom implementation of the normal distribution quantile function (inverse CDF)\n",
    "  def normalQuantile(p: Double): Double = {\n",
    "    val standardNormal = new NormalDistribution(0, 1)\n",
    "    standardNormal.inverseCumulativeProbability(p)\n",
    "  }\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "5114ca65-ac44-413a-b39c-210eb9171fc7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "data2 = [response: double, treatment: int ... 1 more field]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "[response: double, treatment: int ... 1 more field]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val data2 = data.withColumn(\"treatment_str\", col(\"treatment\").cast(\"string\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "98635329-cbeb-44f8-a278-c9c4297da8a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+-------------+\n",
      "|response|treatment|treatment_str|\n",
      "+--------+---------+-------------+\n",
      "|     5.1|        0|            0|\n",
      "|     4.9|        0|            0|\n",
      "|     5.0|        0|            0|\n",
      "|     5.2|        0|            0|\n",
      "|     5.3|        0|            0|\n",
      "|     6.1|        1|            1|\n",
      "|     6.3|        1|            1|\n",
      "|     6.5|        1|            1|\n",
      "|     6.2|        1|            1|\n",
      "|     6.4|        1|            1|\n",
      "+--------+---------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "data2.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "897b4388-b5d8-42ca-a3f8-7fa138a375e4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tTest = (11.999999999999996,0.0,1.2000000000000002,(1.0040036015459948,1.3959963984540056))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "(11.999999999999996,0.0,1.2000000000000002,(1.0040036015459948,1.3959963984540056))"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val tTest = TwoSample.tTestDf(data2, \"treatment_str\", \"response\", \"0\", \"1\", 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1f5b4142-24c4-4387-b7e6-2f74110af861",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "zeroTrimU = (2.7386127875258306,0.0061698993205441255,12.5,(3.5540292814142145,21.445970718585784))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "(2.7386127875258306,0.0061698993205441255,12.5,(3.5540292814142145,21.445970718585784))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val zeroTrimU = TwoSample.zeroTrimmedUDf(data2, \"treatment_str\", \"response\", \"0\", \"1\", 0.05)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e49f06bf-833a-4896-b6dd-b0e2bc8f47f6",
   "metadata": {},
   "source": [
    "## more test with zero"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "ed66a1cd-5640-4cb6-8960-8e44a1b9f4a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dataWithZerosUnequal = [response: double, group: string]\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------+---------+\n",
      "|response|    group|\n",
      "+--------+---------+\n",
      "|     0.0|  control|\n",
      "|     0.0|  control|\n",
      "|     5.0|  control|\n",
      "|     5.2|  control|\n",
      "|     5.3|  control|\n",
      "|     0.0|treatment|\n",
      "|     6.3|treatment|\n",
      "|     6.5|treatment|\n",
      "|     0.0|treatment|\n",
      "|     6.4|treatment|\n",
      "|     6.6|treatment|\n",
      "|     6.7|treatment|\n",
      "+--------+---------+\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[response: double, group: string]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val dataWithZerosUnequal = Seq(\n",
    "  (0.0, \"control\"), (0.0, \"control\"), (5.0, \"control\"), (5.2, \"control\"), (5.3, \"control\"), // control group with zeros\n",
    "  (0.0, \"treatment\"), (6.3, \"treatment\"), (6.5, \"treatment\"), (0.0, \"treatment\"), (6.4, \"treatment\"), (6.6, \"treatment\"), (6.7, \"treatment\") // treatment group with zeros and larger sample size\n",
    ").toDF(\"response\", \"group\")\n",
    "\n",
    "// Show the generated data\n",
    "dataWithZerosUnequal.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "6d0589dc-03f6-4887-a441-25676cdb6012",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "zeroTrimU = (2.1293281415589513,0.03322712107286785,10.0,(0.7953877737928181,19.204612226207182))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "(2.1293281415589513,0.03322712107286785,10.0,(0.7953877737928181,19.204612226207182))"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val zeroTrimU = TwoSample.zeroTrimmedUDf(dataWithZerosUnequal, \"group\", \"response\", \"control\", \"treatment\", 0.05)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd227287-e9cc-4c30-91de-b2fe32f2e7fb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "68c1964f-8ff6-4944-9c6d-ffe66c998c3b",
   "metadata": {},
   "source": [
    "## RDD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "baa5f93c-5630-41d7-87c8-55fcf6ae1298",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "xRdd = MapPartitionsRDD[192] at map at <console>:47\n",
       "yRdd = MapPartitionsRDD[198] at map at <console>:48\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "MapPartitionsRDD[198] at map at <console>:48"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "// Extract x (control group) and y (treatment group) as RDDs\n",
    "val xRdd = dataWithZerosUnequal.filter(col(\"group\") === \"control\").select(\"response\").rdd.map(row => row.getDouble(0))\n",
    "val yRdd = dataWithZerosUnequal.filter(col(\"group\") === \"treatment\").select(\"response\").rdd.map(row => row.getDouble(0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "e7267ab4-54ac-4e23-8b2f-d4845fbccfb5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "zeroTrimU = (2.1293281415589513,0.03322712107286785,1.0,(0.5397693886896409,1.4602306113103591))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "(2.1293281415589513,0.03322712107286785,1.0,(0.5397693886896409,1.4602306113103591))"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val zeroTrimU = TwoSample.zeroTrimmedU(xRdd, yRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a6f23bed-38b3-4341-ab74-8e4c2e7a480d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "mwU = (1.8675952687646453,0.06181850640046682,0.8285714285714286,(0.48374930510628805,1.173393552036569))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "(1.8675952687646453,0.06181850640046682,0.8285714285714286,(0.48374930510628805,1.173393552036569))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val mwU = TwoSample.mwU(xRdd, yRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "c1ffcf4a-dfca-4b4e-a9a3-c79de9f6c89e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "t = (-0.9724839617578134,0.33080983990375046,1.5428571428571431,(-1.5666485685012623,4.652362854215548))\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data",
     "source": "user"
    },
    {
     "data": {
      "text/plain": [
       "(-0.9724839617578134,0.33080983990375046,1.5428571428571431,(-1.5666485685012623,4.652362854215548))"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val t = TwoSample.tTest(xRdd, yRdd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fb27e91-3c72-42ce-91f7-251c4eed5eb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "apache_toree_scala - Scala",
   "language": "scala",
   "name": "apache_toree_scala_scala"
  },
  "language_info": {
   "codemirror_mode": "text/x-scala",
   "file_extension": ".scala",
   "mimetype": "text/x-scala",
   "name": "scala",
   "pygments_lexer": "scala",
   "version": "2.12.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
